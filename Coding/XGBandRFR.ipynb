{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Home Depot Product Search Relevance\n",
    "---------------------------------------------------------------\n",
    "Objective: Rate The Relevance Of A Search Result (Search Term -> Product Result)\n",
    "Category: Home improvement, gardening, construction and do-it-yourself projects\n",
    "URL: https://www.kaggle.com/c/home-depot-product-search-relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Creation\n",
    "---------------------------------------------------------------\n",
    "* [x] Stemming\n",
    "* [x] Replaces\n",
    "* [x] Counts\n",
    "* [x] Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../input/train.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3afddcdf0d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#import enchant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf_pro_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/product_descriptions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[1;32m    472\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3173)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5912)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../input/train.csv does not exist"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "#from nltk.stem.snowball import SnowballStemmer #0.003 improvement but takes twice as long as PorterStemmer\n",
    "#stemmer = SnowballStemmer('english')\n",
    "import re\n",
    "#import enchant\n",
    "\n",
    "df_train = pd.read_csv('../input/train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('../input/test.csv', encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv('../input/product_descriptions.csv')\n",
    "df_attr = pd.read_csv('../input/attributes.csv')\n",
    "df_brand = df_attr[df_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\n",
    "num_train = df_train.shape[0]\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_brand, how='left', on='product_uid')\n",
    "print(\"--- Files Loaded: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "#stop_w = ['for', 'xbi', 'and', 'in', 'th','on','sku','with','what','from','that','less','er','ing'] #'electr','paint','pipe','light','kitchen','wood','outdoor','door','bathroom'\n",
    "strNum = {'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':0}\n",
    "f = open('spelling.txt','r')\n",
    "zspell = {}\n",
    "for line in f:\n",
    "    a, b = line.strip(\"\\n\").split(\"|\")\n",
    "    zspell[a]=b\n",
    "f.close()\n",
    "\n",
    "def str_stem(s): \n",
    "    if isinstance(s, str):\n",
    "        s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) #Split words with a.A\n",
    "        s = s.lower()\n",
    "        s = s.replace(\"  \",\" \")\n",
    "        s = re.sub(r\"([0-9])( *),( *)([0-9])\", r\"\\1\\4\", s)\n",
    "        s = s.replace(\",\",\" \")\n",
    "        s = s.replace(\"$\",\" \")\n",
    "        s = s.replace(\"?\",\" \")\n",
    "        s = s.replace(\"-\",\" \")\n",
    "        s = s.replace(\"//\",\"/\")\n",
    "        s = s.replace(\"..\",\".\")\n",
    "        s = s.replace(\" / \",\" \")\n",
    "        s = s.replace(\" \\\\ \",\" \")\n",
    "        s = s.replace(\".\",\" . \")\n",
    "        s = s.replace(\"   \",\" \")\n",
    "        s = s.replace(\"  \",\" \").strip(\" \")\n",
    "        s = re.sub(r\"(.*)\\.$\", r\"\\1\", s) #end period\n",
    "        s = re.sub(r\"(.*)\\/$\", r\"\\1\", s) #end period\n",
    "        s = re.sub(r\"^\\.(.*)\", r\"\\1\", s) #start period\n",
    "        s = re.sub(r\"^\\/(.*)\", r\"\\1\", s) #start slash\n",
    "        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n",
    "        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n",
    "        s = s.replace(\" x \",\" xbi \")\n",
    "        s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "        s = re.sub(r\"([a-z])( *)/( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "        s = s.replace(\"*\",\" xbi \")\n",
    "        s = s.replace(\" by \",\" xbi \")\n",
    "        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n",
    "        s = s.replace(\"°\",\" degrees \")\n",
    "        s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n",
    "        s = s.replace(\" v \",\" volts \")\n",
    "        s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n",
    "        s = s.replace(\"  \",\" \")\n",
    "        s = s.replace(\" . \",\" \")\n",
    "        #s = (\" \").join([z for z in s.split(\" \") if z not in stop_w])\n",
    "        s = (\" \").join([str(strNum[z]) if z in strNum else z for z in s.split(\" \")])\n",
    "        s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
    "        s = s.lower()\n",
    "        s = (\" \").join([str(zspell[z]) if z in zspell else z for z in s.split(\" \")])\n",
    "        return s\n",
    "    else:\n",
    "        return \"null\"\n",
    "\n",
    "def seg_words(str1, str2):\n",
    "    str2 = str2.lower()\n",
    "    str2 = re.sub(\"[^a-z0-9./]\",\" \", str2)\n",
    "    str2 = [z for z in set(str2.split()) if len(z)>2]\n",
    "    words = str1.lower().split(\" \")\n",
    "    s = []\n",
    "    for word in words:\n",
    "        if len(word)>3:\n",
    "            s1 = []\n",
    "            s1 += segmentit(word,str2,True)\n",
    "            if len(s)>1:\n",
    "                s += [z for z in s1 if z not in ['er','ing','s','less'] and len(z)>1]\n",
    "            else:\n",
    "                s.append(word)\n",
    "        else:\n",
    "            s.append(word)\n",
    "    return (\" \".join(s))\n",
    "\n",
    "def segmentit(s, txt_arr, t):\n",
    "    st = s\n",
    "    r = []\n",
    "    for j in range(len(s)):\n",
    "        for word in txt_arr:\n",
    "            if word == s[:-j]:\n",
    "                r.append(s[:-j])\n",
    "                #print(s[:-j],s[len(s)-j:])\n",
    "                s=s[len(s)-j:]\n",
    "                r += segmentit(s, txt_arr, False)\n",
    "    if t:\n",
    "        i = len((\"\").join(r))\n",
    "        if not i==len(st):\n",
    "            r.append(st[i:])\n",
    "    return r\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "    words, cnt = str1.split(), 0\n",
    "    for word in words:\n",
    "        if str2.find(word)>=0:\n",
    "            cnt+=1\n",
    "    return cnt\n",
    "\n",
    "def str_whole_word(str1, str2, i_):\n",
    "    cnt = 0\n",
    "    while i_ < len(str2):\n",
    "        i_ = str2.find(str1, i_)\n",
    "        if i_ == -1:\n",
    "            return cnt\n",
    "        else:\n",
    "            cnt += 1\n",
    "            i_ += len(str1)\n",
    "    return cnt\n",
    "\n",
    "#comment out the lines below use df_all.csv for further grid search testing\n",
    "#if adding features consider any drops on the 'cust_regression_vals' class\n",
    "#*** would be nice to have a file reuse option or script chaining option on Kaggle Scripts ***\n",
    "df_all['search_term'] = df_all['search_term'].map(lambda x:str_stem(x))\n",
    "df_all['product_title'] = df_all['product_title'].map(lambda x:str_stem(x))\n",
    "df_all['product_description'] = df_all['product_description'].map(lambda x:str_stem(x))\n",
    "df_all['brand'] = df_all['brand'].map(lambda x:str_stem(x))\n",
    "print(\"--- Stemming: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['product_info'] = df_all['search_term']+\"\\t\"+df_all['product_title'] +\"\\t\"+df_all['product_description']\n",
    "print(\"--- Prod Info: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_title'] = df_all['product_title'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_description'] = df_all['product_description'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_brand'] = df_all['brand'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "print(\"--- Len of: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['search_term'] = df_all['product_info'].map(lambda x:seg_words(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "#print(\"--- Search Term Segment: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['query_in_title'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
    "df_all['query_in_description'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[2],0))\n",
    "print(\"--- Query In: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['query_last_word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0].split(\" \")[-1],x.split('\\t')[1]))\n",
    "df_all['query_last_word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0].split(\" \")[-1],x.split('\\t')[2]))\n",
    "print(\"--- Query Last Word In: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "df_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[2]))\n",
    "df_all['ratio_title'] = df_all['word_in_title']/df_all['len_of_query']\n",
    "df_all['ratio_description'] = df_all['word_in_description']/df_all['len_of_query']\n",
    "df_all['attr'] = df_all['search_term']+\"\\t\"+df_all['brand']\n",
    "df_all['word_in_brand'] = df_all['attr'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "df_all['ratio_brand'] = df_all['word_in_brand']/df_all['len_of_brand']\n",
    "df_brand = pd.unique(df_all.brand.ravel())\n",
    "d={}\n",
    "i = 1000\n",
    "for s in df_brand:\n",
    "    d[s]=i\n",
    "    i+=3\n",
    "df_all['brand_feature'] = df_all['brand'].map(lambda x:d[x])\n",
    "df_all['search_term_feature'] = df_all['search_term'].map(lambda x:len(x))\n",
    "df_all.to_csv('df_all.csv')\n",
    "print(\"--- Features Set: %s minutes ---\" % round(((time.time() - start_time)/60),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Creation\n",
    "---------------------------------------------------------------\n",
    "* [x] Unstemmed Search Term Dictionary\n",
    "* [x] Search Term Words in Features (takes long to run)\n",
    "* [x] Search Term Word Counts Across Test & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.metrics import edit_distance\n",
    "import re\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "    str2 = str2.lower().split(\" \")\n",
    "    if str1 in str2:\n",
    "        cnt=1\n",
    "    else:\n",
    "        cnt=0\n",
    "    return cnt\n",
    "\n",
    "def str_common_word2(str1, str2):\n",
    "    str2 = str(str2).lower()\n",
    "    if str2.find(str1)>=0:\n",
    "        cnt=1\n",
    "    else:\n",
    "        cnt=0\n",
    "    return cnt\n",
    "\n",
    "df_train = pd.read_csv('../input/train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('../input/test.csv', encoding=\"ISO-8859-1\")\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = df_all[['product_uid','search_term','product_title']]\n",
    "df_all.reset_index(inplace=True)\n",
    "\n",
    "df_prod = pd.read_csv('../input/product_descriptions.csv').fillna(\" \")\n",
    "df_attr = pd.read_csv('../input/attributes.csv').fillna(\" \")\n",
    "print(\"--- Files Loaded: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "d_prod_query = {}\n",
    "for i in range(len(df_all)):\n",
    "    b_ = str(df_all['product_uid'][i])\n",
    "    if b_ not in d_prod_query:\n",
    "        d_prod_query[b_] = [list(set(str(df_all['search_term'][i]).lower().split(\" \"))), \n",
    "                            str(df_all['product_title'][i]).lower(),\n",
    "                            str(df_prod.loc[df_prod['product_uid'] == df_all['product_uid'][i]]['product_description'].iloc[0]).lower()]\n",
    "    else:\n",
    "        d_prod_query[b_][0] = list(set(d_prod_query[b_][0] + list(set(str(df_all['search_term'][i]).lower().split(\" \")))))\n",
    "\n",
    "f = open(\"dictionary.txt\", \"w\")\n",
    "f.write(str(d_prod_query))\n",
    "f.close()\n",
    "\n",
    "print(\"--- Product & Search Term Dictionary: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "#stop_ = list(text.ENGLISH_STOP_WORDS)\n",
    "stop_ = []\n",
    "d={}\n",
    "for i in d_prod_query:\n",
    "    a = d_prod_query[i][0]\n",
    "    df_gen_attr = df_attr.loc[df_attr['product_uid'] == int(i)]\n",
    "    for b_ in a:\n",
    "        if len(b_)>0:\n",
    "            col_lst = []\n",
    "            for j in range(len(df_gen_attr)):\n",
    "                if str_common_word(b_, df_gen_attr['value'].iloc[j])>0:\n",
    "                    col_lst.append(df_gen_attr['name'].iloc[j])\n",
    "            #if b_ not in stop_:\n",
    "            if b_ not in d:\n",
    "                d[b_] = [1,str_common_word(b_, d_prod_query[i][1]),str_common_word2(b_, d_prod_query[i][1]),col_lst[:]]\n",
    "            else:\n",
    "                d[b_][0] += 1\n",
    "                d[b_][1] += str_common_word(b_, d_prod_query[i][1])\n",
    "                d[b_][2] += str_common_word2(b_, d_prod_query[i][1])\n",
    "                d[b_][3] =  list(set(d[b_][3] + col_lst))\n",
    "\n",
    "ds2 = pd.DataFrame.from_dict(d,orient='index')\n",
    "ds2.columns = ['count','in title 1','in title 2','attribute type']\n",
    "ds2 = ds2.sort_values(by=['count'], ascending=[False])\n",
    "\n",
    "f = open(\"word_review_v2.csv\", \"w\")\n",
    "f.write(\"word|count|in title 1|in title 2|attribute type\\n\")\n",
    "for i in range(len(ds2)):\n",
    "    f.write(ds2.index[i] + \"|\" + str(ds2[\"count\"][i]) + \"|\" + str(ds2[\"in title 1\"][i]) + \"|\" + str(ds2[\"in title 2\"][i]) + \"|\" + str(ds2[\"attribute type\"][i]) + \"\\n\")\n",
    "f.close()\n",
    "print(\"--- File Created: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Created: 0.73 minutes ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.metrics import edit_distance\n",
    "import re\n",
    "\n",
    "df_train = pd.read_csv('../input/train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('../input/test.csv', encoding=\"ISO-8859-1\")\n",
    "df_temp = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "num_train = df_train.shape[0]\n",
    "df_all = pd.read_csv('df_all.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "\n",
    "#v1\n",
    "dtest = pd.read_csv('word_review_v2.csv', encoding=\"ISO-8859-1\", index_col=0, sep='|').to_dict('index')\n",
    "dm_attr = [[z,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0] for z in range(len(df_temp))]\n",
    "for a in range(len(df_temp)):\n",
    "    b = [z for z in str(df_temp.search_term[a]).split(\" \") if len(z)>0]\n",
    "    for c in range(1,len(b)+1):\n",
    "        d = str(b[c-1]).lower()\n",
    "        d = re.sub(r\"([0-9]),([0-9])\", r\"\\1\\2\", d)\n",
    "        if d in dtest:\n",
    "            dm_attr[a][c-1] = dtest[d]['in title 1'] / dtest[d]['count']\n",
    "        else:\n",
    "            dm_attr[a][c-1] = 0.0\n",
    "df_dm_attr = pd.DataFrame(dm_attr)\n",
    "df_dm_attr.columns = ['ft01','ft02','ft03','ft04','ft05','ft06','ft07','ft08','ft09','ft10','ft11','ft12','ft13','ft14']\n",
    "df_all = pd.concat([df_all, df_dm_attr], axis=1)\n",
    "#v2\n",
    "dm_attr = [[z,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0] for z in range(len(df_temp))]\n",
    "for a in range(len(df_temp)):\n",
    "    b = [z for z in str(df_temp.search_term[a]).split(\" \") if len(z)>0]\n",
    "    for c in range(1,len(b)+1):\n",
    "        d = str(b[c-1]).lower()\n",
    "        d = re.sub(r\"([0-9]),([0-9])\", r\"\\1\\2\", d)\n",
    "        if d in dtest:\n",
    "            dm_attr[a][c-1] = dtest[d]['in title 2'] / dtest[d]['count']\n",
    "        else:\n",
    "            dm_attr[a][c-1] = 0.0\n",
    "df_dm_attr = pd.DataFrame(dm_attr)\n",
    "df_dm_attr.columns = ['ftx01','ftx02','ftx03','ftx04','ftx05','ftx06','ftx07','ftx08','ftx09','ftx10','ftx11','ftx12','ftx13','ftx14']\n",
    "df_all = pd.concat([df_all, df_dm_attr], axis=1)\n",
    "#v3\n",
    "dm_attr = [[z,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0] for z in range(len(df_temp))]\n",
    "for a in range(len(df_temp)):\n",
    "    b = [z for z in str(df_temp.search_term[a]).split(\" \") if len(z)>0]\n",
    "    for c in range(1,len(b)+1):\n",
    "        d = str(b[c-1]).lower()\n",
    "        d = re.sub(r\"([0-9]),([0-9])\", r\"\\1\\2\", d)\n",
    "        if d in dtest:\n",
    "            dm_attr[a][c-1] = len(dtest[d]['attribute type'].split(\",\"))\n",
    "        else:\n",
    "            dm_attr[a][c-1] = 0.0\n",
    "df_dm_attr = pd.DataFrame(dm_attr)\n",
    "df_dm_attr.columns = ['ftz01','ftz02','ftz03','ftz04','ftz05','ftz06','ftz07','ftz08','ftz09','ftz10','ftz11','ftz12','ftz13','ftz14']\n",
    "df_all = pd.concat([df_all, df_dm_attr], axis=1)\n",
    "\n",
    "df_all.to_csv('df_all2.csv')\n",
    "print(\"--- File Created: %s minutes ---\" % round(((time.time() - start_time)/60),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Feature Creation\n",
    "---------------------------------------------------------------\n",
    "* [x] Stemmed Search Term Dictionary\n",
    "* [x] Search Term Words in Features (takes long to run)\n",
    "* [x] Search Term Word Counts Across Test & Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Files Loaded: 0.02 minutes ---\n",
      "--- File Created: 3.54 minutes ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import re\n",
    "\n",
    "def str_stem(s): \n",
    "    if isinstance(s, str):\n",
    "        s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) #Split words with a.A\n",
    "        s = s.lower()\n",
    "        s = s.replace(\"  \",\" \")\n",
    "        s = re.sub(r\"([0-9])( *),( *)([0-9])\", r\"\\1\\4\", s)\n",
    "        s = s.replace(\",\",\" \")\n",
    "        s = s.replace(\"$\",\" \")\n",
    "        s = s.replace(\"?\",\" \")\n",
    "        s = s.replace(\"-\",\" \")\n",
    "        s = s.replace(\"//\",\"/\")\n",
    "        s = s.replace(\"..\",\".\")\n",
    "        s = s.replace(\" / \",\" \")\n",
    "        s = s.replace(\" \\\\ \",\" \")\n",
    "        s = s.replace(\".\",\" . \")\n",
    "        s = s.replace(\"   \",\" \")\n",
    "        s = s.replace(\"  \",\" \").strip(\" \")\n",
    "        s = re.sub(r\"(.*)\\.$\", r\"\\1\", s) #end period\n",
    "        s = re.sub(r\"(.*)\\/$\", r\"\\1\", s) #end slash\n",
    "        s = re.sub(r\"^\\.(.*)\", r\"\\1\", s) #start period\n",
    "        s = re.sub(r\"^\\/(.*)\", r\"\\1\", s) #start slash\n",
    "        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n",
    "        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n",
    "        s = s.replace(\" x \",\" xbi \")\n",
    "        s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "        s = re.sub(r\"([a-z])( *)/( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "        s = s.replace(\"*\",\" xbi \")\n",
    "        s = s.replace(\" by \",\" xbi \")\n",
    "        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n",
    "        s = s.replace(\"°\",\" degrees \")\n",
    "        s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n",
    "        s = s.replace(\" v \",\" volts \")\n",
    "        s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n",
    "        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n",
    "        s = s.replace(\"  \",\" \")\n",
    "        s = s.replace(\" . \",\" \")\n",
    "        s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
    "        s = s.lower()\n",
    "        return s\n",
    "    else:\n",
    "        return \"null\"\n",
    "\n",
    "df_attr = pd.read_csv('../input/attributes.csv').fillna(\" \")\n",
    "print(\"--- Files Loaded: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "df_attr['value'] = df_attr['value'].map(lambda x:str_stem(str(x)))\n",
    "df_attr.to_csv('attributes_stemmed.csv')\n",
    "print(\"--- File Created: %s minutes ---\" % round(((time.time() - start_time)/60),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Files Loaded: 0.1 minutes ---\n",
      "--- Product & Search Term Dictionary: 0.21 minutes ---\n",
      "--- File Created: 124.74 minutes ---\n",
      "--- V1 Complete: 124.88 minutes ---\n",
      "--- V2 Complete: 124.95 minutes ---\n",
      "--- V3 Complete: 125.05 minutes ---\n",
      "--- File Created: 125.52 minutes ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "    str2 = str2.lower().split(\" \")\n",
    "    if str1 in str2:\n",
    "        cnt=1\n",
    "    else:\n",
    "        cnt=0\n",
    "    return cnt\n",
    "\n",
    "def str_common_word2(str1, str2):\n",
    "    str2 = str(str2).lower()\n",
    "    if str2.find(str1)>=0:\n",
    "        cnt=1\n",
    "    else:\n",
    "        cnt=0\n",
    "    return cnt\n",
    "\n",
    "df_all = pd.read_csv('df_all2.csv', encoding=\"ISO-8859-1\")\n",
    "df_all = df_all[['product_uid','search_term','product_title','product_description']]\n",
    "df_all.reset_index(inplace=True)\n",
    "df_attr = pd.read_csv('attributes_stemmed.csv').fillna(\" \")\n",
    "print(\"--- Files Loaded: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "d_prod_query = {}\n",
    "for i in range(len(df_all)):\n",
    "    b_ = str(df_all['product_uid'][i])\n",
    "    if b_ not in d_prod_query:\n",
    "        d_prod_query[b_] = [list(set(str(df_all['search_term'][i]).lower().split(\" \"))), \n",
    "                            str(df_all['product_title'][i]),\n",
    "                            str(df_all['product_description'][i])]\n",
    "    else:\n",
    "        d_prod_query[b_][0] = list(set(d_prod_query[b_][0] + list(set(str(df_all['search_term'][i]).lower().split(\" \")))))\n",
    "\n",
    "f = open(\"dictionary_stemmed.txt\", \"w\")\n",
    "f.write(str(d_prod_query))\n",
    "f.close()\n",
    "\n",
    "print(\"--- Product & Search Term Dictionary: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "#stop_ = list(text.ENGLISH_STOP_WORDS)\n",
    "stop_ = []\n",
    "d={}\n",
    "for i in d_prod_query:\n",
    "    a = d_prod_query[i][0]\n",
    "    df_gen_attr = df_attr.loc[df_attr['product_uid'] == str(i)+\".0\"]\n",
    "    for b_ in a:\n",
    "        if len(b_)>0:\n",
    "            col_lst = []\n",
    "            for j in range(len(df_gen_attr)):\n",
    "                if str_common_word(b_, df_gen_attr['value'].iloc[j])>0:\n",
    "                    col_lst.append(df_gen_attr['name'].iloc[j])\n",
    "            if b_ not in d:\n",
    "                d[b_] = [1,str_common_word(b_, d_prod_query[i][1]),str_common_word2(b_, d_prod_query[i][1]),col_lst[:]]\n",
    "            else:\n",
    "                d[b_][0] += 1\n",
    "                d[b_][1] += str_common_word(b_, d_prod_query[i][1])\n",
    "                d[b_][2] += str_common_word2(b_, d_prod_query[i][1])\n",
    "                d[b_][3] =  list(set(d[b_][3] + col_lst))\n",
    "\n",
    "ds2 = pd.DataFrame.from_dict(d,orient='index')\n",
    "ds2.columns = ['count','in title 1','in title 2','attribute type']\n",
    "ds2 = ds2.sort_values(by=['count'], ascending=[False])\n",
    "\n",
    "f = open(\"word_review_stemmed.csv\", \"w\")\n",
    "f.write(\"word|count|in title 1|in title 2|attribute type\\n\")\n",
    "for i in range(len(ds2)):\n",
    "    f.write(ds2.index[i] + \"|\" + str(ds2[\"count\"][i]) + \"|\" + str(ds2[\"in title 1\"][i]) + \"|\" + str(ds2[\"in title 2\"][i]) + \"|\" + str(ds2[\"attribute type\"][i]) + \"\\n\")\n",
    "f.close()\n",
    "print(\"--- File Created: %s minutes ---\" % round(((time.time() - start_time)/60),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- V1 Complete: 0.17 minutes ---\n",
      "--- V2 Complete: 0.24 minutes ---\n",
      "--- V3 Complete: 0.34 minutes ---\n",
      "--- File Created: 0.86 minutes ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.metrics import edit_distance\n",
    "import re\n",
    "\n",
    "df_all = pd.read_csv('df_all2.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "df_temp = df_all[:]\n",
    "\n",
    "#v1\n",
    "dtest = pd.read_csv('word_review_stemmed.csv', encoding=\"ISO-8859-1\", index_col=0, sep='|').to_dict('index')\n",
    "dm_attr = [[z,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0] for z in range(len(df_temp))]\n",
    "for a in range(len(df_temp)):\n",
    "    b = [z for z in str(df_temp.search_term[a]).split(\" \") if len(z)>0]\n",
    "    for c in range(1,len(b)+1):\n",
    "        d = str(b[c-1]).lower()\n",
    "        #d = re.sub(r\"([0-9]),([0-9])\", r\"\\1\\2\", d)\n",
    "        if d in dtest:\n",
    "            dm_attr[a][c-1] = dtest[d]['in title 1'] / dtest[d]['count']\n",
    "        else:\n",
    "            dm_attr[a][c-1] = 0.0\n",
    "df_dm_attr = pd.DataFrame(dm_attr)\n",
    "df_dm_attr.columns = ['sft01','sft02','sft03','sft04','sft05','sft06','sft07','sft08','sft09','sft10','sft11','sft12','sft13','sft14','sft101','sft102','sft103','sft104','sft105','sft106','sft107','sft108','sft109','sft110','sft111','sft112','sft113','sft114']\n",
    "df_all = pd.concat([df_all, df_dm_attr], axis=1)\n",
    "print(\"--- V1 Complete: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "#v2\n",
    "dm_attr = [[z,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0] for z in range(len(df_temp))]\n",
    "for a in range(len(df_temp)):\n",
    "    b = [z for z in str(df_temp.search_term[a]).split(\" \") if len(z)>0]\n",
    "    for c in range(1,len(b)+1):\n",
    "        d = str(b[c-1]).lower()\n",
    "        #d = re.sub(r\"([0-9]),([0-9])\", r\"\\1\\2\", d)\n",
    "        if d in dtest:\n",
    "            dm_attr[a][c-1] = dtest[d]['in title 2'] / dtest[d]['count']\n",
    "        else:\n",
    "            dm_attr[a][c-1] = 0.0\n",
    "df_dm_attr = pd.DataFrame(dm_attr)\n",
    "df_dm_attr.columns = ['sftx01','sftx02','sftx03','sftx04','sftx05','sftx06','sftx07','sftx08','sftx09','sftx10','sftx11','sftx12','sftx13','sftx14','sftx101','sftx102','sftx103','sftx104','sftx105','sftx106','sftx107','sftx108','sftx109','sftx110','sftx111','sftx112','sftx113','sftx114']\n",
    "df_all = pd.concat([df_all, df_dm_attr], axis=1)\n",
    "print(\"--- V2 Complete: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "#v3\n",
    "[[z,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0,-99.0] for z in range(len(df_temp))]\n",
    "for a in range(len(df_temp)):\n",
    "    b = [z for z in str(df_temp.search_term[a]).split(\" \") if len(z)>0]\n",
    "    for c in range(1,len(b)+1):\n",
    "        d = str(b[c-1]).lower()\n",
    "        #d = re.sub(r\"([0-9]),([0-9])\", r\"\\1\\2\", d)\n",
    "        if d in dtest:\n",
    "            dm_attr[a][c-1] = len(dtest[d]['attribute type'].split(\",\"))\n",
    "        else:\n",
    "            dm_attr[a][c-1] = 0.0\n",
    "df_dm_attr = pd.DataFrame(dm_attr)\n",
    "df_dm_attr.columns = ['sftz01','sftz02','sftz03','sftz04','sftz05','sftz06','sftz07','sftz08','sftz09','sftz10','sftz11','sftz12','sftz13','sftz14','sftz101','sftz102','sftz103','sftz104','sftz105','sftz106','sftz107','sftz108','sftz109','sftz110','sftz111','sftz112','sftz113','sftz114']\n",
    "df_all = pd.concat([df_all, df_dm_attr], axis=1)\n",
    "print(\"--- V3 Complete: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all.to_csv('df_all3.csv')\n",
    "print(\"--- File Created: %s minutes ---\" % round(((time.time() - start_time)/60),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Search Terms and Product Title\n",
    "---------------------------------------------------------------\n",
    "* [x] Search Term and Product Title Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Files Loaded: 0.11 minutes ---\n",
      "--- Feature Created: 0.12 minutes ---\n",
      "--- File Created: 0.63 minutes ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def str_join_words(str1, str2):\n",
    "    s=(\" \").join([\"q_\"+ z for z in str1.split(\" \")])  + \" \" + str2\n",
    "    return s\n",
    "\n",
    "df_all = pd.read_csv('df_all3.csv', index_col=0, encoding=\"ISO-8859-1\")\n",
    "print(\"--- Files Loaded: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all['search_and_prod_info'] = df_all['product_info'].map(lambda x:str_join_words(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "print(\"--- Feature Created: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "df_all.to_csv('df_all4.csv')\n",
    "print(\"--- File Created: %s minutes ---\" % round(((time.time() - start_time)/60),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor\n",
    "---------------------------------------------------------------\n",
    "* [x] Use Transformer Mixin for custom TFI-DF Feature\n",
    "* [x] Ordered Test Set for Improved CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features Set: 0.26 minutes ---\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   2 | elapsed:   28.2s remaining:   -9.4s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   28.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   20.5s finished\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   20.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   22.8s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by grid search:\n",
      "{'rfr__max_depth': 25, 'rfr__max_features': 25}\n",
      "Best CV score:\n",
      "-0.459252230932\n",
      "-0.00675837312355\n",
      "----------------------------------\n",
      "{'rfr__max_depth': 25, 'rfr__max_features': 25} -0.459252230932\n",
      "[-0.45844049 -0.46006399]\n",
      "----------------------------------\n",
      "1.1637[CV] rfr__max_depth=25, rfr__max_features=25 .........................\n",
      "[CV] rfr__max_depth=25, rfr__max_features=25 .........................\n",
      "[CV]  rfr__max_depth=25, rfr__max_features=25, score=-0.458440 -  26.7s[CV]  rfr__max_depth=25, rfr__max_features=25, score=-0.460064 -  26.3s\n",
      "\n",
      " 2.96460247493 1.0 3.0\n",
      "--- Training & Testing: 1.46 minutes ---\n",
      "------------------------------------\n",
      "34 [ 0.02878965  0.01339533  0.01490682  0.00286299  0.01778018  0.01022486\n",
      "  0.01416061  0.00406367  0.09151926  0.03845397  0.01017751  0.01983378\n",
      "  0.01110692  0.00934903  0.00538062  0.00841997  0.00795578  0.00611195\n",
      "  0.00328462  0.00612756  0.00718094  0.00573984  0.01068205  0.00890186\n",
      "  0.00640731  0.00377616  0.00875582  0.00854036  0.00671156  0.00366829\n",
      "  0.00659473  0.0068872   0.00483486  0.00305346]\n",
      "------------------------------------\n",
      "50 [ 0.0102242   0.01091576  0.01060589  0.01000411  0.00998476  0.0105463\n",
      "  0.01006158  0.01037614  0.00987547  0.01117376  0.01587736  0.01495395\n",
      "  0.01389924  0.01539812  0.01518897  0.01510599  0.01453228  0.01533296\n",
      "  0.01676135  0.01627236  0.01115538  0.01040325  0.00811703  0.00896514\n",
      "  0.01180175  0.00738599  0.00811189  0.01010626  0.00795118  0.00923422\n",
      "  0.00258887  0.00403293  0.00447408  0.00510423  0.00599564  0.00570825\n",
      "  0.00591725  0.00597037  0.0062363   0.00695263  0.02149044  0.01578328\n",
      "  0.01719467  0.02073625  0.01930245  0.01614957  0.01613326  0.01643989\n",
      "  0.02001735  0.01781017]\n",
      "------------------------------------\n",
      "['query_in_title', 'sftz04']\n"
     ]
    }
   ],
   "source": [
    "#RFR\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import pipeline #model_selection\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import random\n",
    "random.seed(2016)\n",
    "\n",
    "df_train = pd.read_csv('../input/train.csv', encoding=\"ISO-8859-1\")\n",
    "num_train = df_train.shape[0]\n",
    "df_all = pd.read_csv('df_all4.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "train = df_all.iloc[:num_train]\n",
    "test = df_all.iloc[num_train:]\n",
    "id_test = test['id']\n",
    "#balance train\n",
    "import sqlite3\n",
    "c = sqlite3.connect(':memory:')\n",
    "#c = sqlite3.connect('temp.db')\n",
    "train.to_sql('t',c)\n",
    "train = pd.read_sql('select * from t order by relevance desc, product_uid asc', c, index_col=['index'])\n",
    "df_even = train.iloc[::2]  # even\n",
    "df_odd = train.iloc[1::2]  # odd\n",
    "train = pd.concat((df_even, df_odd), axis=0, ignore_index=True)\n",
    "train = train.reset_index(drop=True)\n",
    "y_train = train['relevance']\n",
    "print(\"--- Features Set: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "RMSE  = make_scorer(fmean_squared_error, greater_is_better=False)\n",
    "\n",
    "class cust_regression_vals(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, hd_searches):\n",
    "        d_col_drops=['id','relevance','search_term','product_title','product_description','product_info','attr','brand','search_and_prod_info','len_of_query', 'len_of_brand', 'query_in_description', 'word_in_brand', 'ratio_brand', 'ft04', 'ft05', 'ftx05', 'ftz04', 'ftz05', 'sft05', 'sftx05', 'sftz05','ft06', 'ft07', 'ft08', 'ft09', 'ft10', 'ft11', 'ft12', 'ft13', 'ft14', 'ftx06', 'ftx07', 'ftx08', 'ftx09', 'ftx10', 'ftx11', 'ftx12', 'ftx13', 'ftx14', 'ftz06', 'ftz07', 'ftz08', 'ftz09', 'ftz10', 'ftz11', 'ftz12', 'ftz13', 'ftz14', 'sft06', 'sft07', 'sft08', 'sft09', 'sft10', 'sft11', 'sft12', 'sft13', 'sft14', 'sft101', 'sft102', 'sft103', 'sft104', 'sft105', 'sft106', 'sft107', 'sft108', 'sft109', 'sft110', 'sft111', 'sft112', 'sft113', 'sft114', 'sftx06', 'sftx07', 'sftx08', 'sftx09', 'sftx10', 'sftx11', 'sftx12', 'sftx13', 'sftx14', 'sftx101', 'sftx102', 'sftx103', 'sftx104', 'sftx105', 'sftx106', 'sftx107', 'sftx108', 'sftx109', 'sftx110', 'sftx111', 'sftx112', 'sftx113', 'sftx114', 'sftz06', 'sftz07', 'sftz08', 'sftz09', 'sftz10', 'sftz11', 'sftz12', 'sftz13', 'sftz14', 'sftz101', 'sftz102', 'sftz103', 'sftz104', 'sftz105', 'sftz106', 'sftz107', 'sftz108', 'sftz109', 'sftz110', 'sftz111', 'sftz112', 'sftz113', 'sftz114']\n",
    "        hd_searches = hd_searches.drop(d_col_drops,axis=1).values\n",
    "        return hd_searches\n",
    "\n",
    "class cust_txt_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key].apply(str)\n",
    "\n",
    "#for ij in range(5):\n",
    "rfr = RandomForestRegressor(n_estimators = 50, n_jobs = -1, random_state = 2016, verbose = 1)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "tsvd = TruncatedSVD(n_components=10, random_state = 2016)\n",
    "tnmf = NMF(n_components=10, random_state = 2016)\n",
    "tpca = PCA(n_components=10)\n",
    "clf = pipeline.Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "                    transformer_list = [\n",
    "                        ('cst',  cust_regression_vals()),  \n",
    "                        ('txt1', pipeline.Pipeline([('s1', cust_txt_col(key='search_term')), ('tfidf1', tfidf), ('tsvd1', tsvd)])),\n",
    "                        ('txt2', pipeline.Pipeline([('s2', cust_txt_col(key='search_and_prod_info')), ('tfidf2', tfidf), ('tsvd2', tsvd)])),\n",
    "                        ('txt3', pipeline.Pipeline([('s3', cust_txt_col(key='search_and_prod_info')), ('tfidf3', tfidf), ('tnmf', tnmf)])),\n",
    "                        ('txt4', pipeline.Pipeline([('s4', cust_txt_col(key='brand')), ('tfidf4', tfidf), ('tsvd4', tsvd)]))\n",
    "                        ],\n",
    "                    transformer_weights = {\n",
    "                        'cst': 1.0,\n",
    "                        'txt1': 0.5,\n",
    "                        'txt2': 0.75,\n",
    "                        'txt3': 0.75,\n",
    "                        'txt4': 0.5\n",
    "                        },\n",
    "                #n_jobs = -1\n",
    "                )), \n",
    "        ('rfr', rfr)])\n",
    "param_grid = {'rfr__max_features': [10], 'rfr__max_depth': [15]}\n",
    "\n",
    "model = GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs = -1, cv = 2, verbose = 20, scoring=RMSE)\n",
    "model.fit(train, y_train.values)\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)\n",
    "print(model.best_score_ + 0.452493857808)\n",
    "print(\"----------------------------------\")\n",
    "for i in range(len(model.grid_scores_)):\n",
    "    print(model.grid_scores_[i][0], model.grid_scores_[i][1])\n",
    "    print(model.grid_scores_[i][2])\n",
    "    print(\"----------------------------------\")\n",
    "    \n",
    "y_pred = model.predict(test)\n",
    "\n",
    "#print(len(y_pred))\n",
    "#pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission_before.csv',index=False)\n",
    "min_y_pred = min(y_pred)\n",
    "max_y_pred = max(y_pred)\n",
    "min_y_train = min(y_train.values)\n",
    "max_y_train = max(y_train.values)\n",
    "print(min_y_pred, max_y_pred, min_y_train, max_y_train)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]<1.0:\n",
    "        y_pred[i] = 1.0\n",
    "    if y_pred[i]>3.0:\n",
    "        y_pred[i] = 3.0\n",
    "    #y_pred[i] = min_y_train + (((y_pred[i] - min_y_pred)/(max_y_pred - min_y_pred))*(max_y_train - min_y_train))\n",
    "pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission_rfr_02.csv',index=False)\n",
    "print(\"--- Training & Testing: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "d_col_drops=['id','relevance','search_term','product_title','product_description','product_info','attr','brand','search_and_prod_info','len_of_query', 'len_of_brand', 'query_in_description', 'word_in_brand', 'ratio_brand', 'ft04', 'ft05', 'ftx05', 'ftz04', 'ftz05', 'sft05', 'sftx05', 'sftz05','ft06', 'ft07', 'ft08', 'ft09', 'ft10', 'ft11', 'ft12', 'ft13', 'ft14', 'ftx06', 'ftx07', 'ftx08', 'ftx09', 'ftx10', 'ftx11', 'ftx12', 'ftx13', 'ftx14', 'ftz06', 'ftz07', 'ftz08', 'ftz09', 'ftz10', 'ftz11', 'ftz12', 'ftz13', 'ftz14', 'sft06', 'sft07', 'sft08', 'sft09', 'sft10', 'sft11', 'sft12', 'sft13', 'sft14', 'sft101', 'sft102', 'sft103', 'sft104', 'sft105', 'sft106', 'sft107', 'sft108', 'sft109', 'sft110', 'sft111', 'sft112', 'sft113', 'sft114', 'sftx06', 'sftx07', 'sftx08', 'sftx09', 'sftx10', 'sftx11', 'sftx12', 'sftx13', 'sftx14', 'sftx101', 'sftx102', 'sftx103', 'sftx104', 'sftx105', 'sftx106', 'sftx107', 'sftx108', 'sftx109', 'sftx110', 'sftx111', 'sftx112', 'sftx113', 'sftx114', 'sftz06', 'sftz07', 'sftz08', 'sftz09', 'sftz10', 'sftz11', 'sftz12', 'sftz13', 'sftz14', 'sftz101', 'sftz102', 'sftz103', 'sftz104', 'sftz105', 'sftz106', 'sftz107', 'sftz108', 'sftz109', 'sftz110', 'sftz111', 'sftz112', 'sftz113', 'sftz114']\n",
    "feature_names = np.array(train.drop(['id','relevance'],axis=1).columns.values.tolist())\n",
    "feature_names = np.array([z for z in feature_names if z not in d_col_drops])\n",
    "importances = model.best_estimator_.named_steps['rfr'].feature_importances_[:len(feature_names)]\n",
    "tfidf_imp = model.best_estimator_.named_steps['rfr'].feature_importances_[len(feature_names):]\n",
    "important_names = feature_names[importances > (np.mean(importances)/4)]\n",
    "drop_names = [a for a in feature_names if a not in important_names]\n",
    "print(\"------------------------------------\")\n",
    "print(len(importances), importances)\n",
    "print(\"------------------------------------\")\n",
    "print(len(tfidf_imp), tfidf_imp)\n",
    "print(\"------------------------------------\")\n",
    "print(drop_names)\n",
    "#train = train.drop(drop_names,axis=1)\n",
    "#test = test.drop(drop_names,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG Boost\n",
    "---------------------------------------------------------------\n",
    "* [x] Use Transformer Mixin for custom TFI-DF Feature\n",
    "* [x] Ordered Test Set for Improved CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features Set: 0.26 minutes ---\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 26.9min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   2 | elapsed: 26.9min remaining:  -538.5s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 26.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Best parameters found by grid search:\n",
      "{'xgb_model__max_depth': 10, 'xgb_model__n_estimators': 2000}\n",
      "Best CV score:\n",
      "-0.45839473211\n",
      "-0.00590087430221\n",
      "----------------------------------\n",
      "{'xgb_model__max_depth': 10, 'xgb_model__n_estimators': 2000} -0.45839473211\n",
      "[-0.45789505 -0.45889443]\n",
      "----------------------------------\n",
      "0.918272[CV] xgb_model__max_depth=10, xgb_model__n_estimators=2000 ...........\n",
      "[CV] xgb_model__max_depth=10, xgb_model__n_estimators=2000 ...........\n",
      "[CV]  xgb_model__max_depth=10, xgb_model__n_estimators=2000, score=-0.457895 -26.9min[CV]  xgb_model__max_depth=10, xgb_model__n_estimators=2000, score=-0.458894 -26.9min\n",
      "\n",
      " 3.13836 1.0 3.0\n",
      "--- Training & Testing: 43.12 minutes ---\n"
     ]
    }
   ],
   "source": [
    "#XGB\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import pipeline #model_selection\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import re\n",
    "import random\n",
    "random.seed(2016)\n",
    "import xgboost as xgb\n",
    "import sqlite3\n",
    "\n",
    "df_train = pd.read_csv('../input/train.csv', encoding=\"ISO-8859-1\")\n",
    "num_train = df_train.shape[0]\n",
    "df_all = pd.read_csv('df_all4.csv', encoding=\"ISO-8859-1\", index_col=0)\n",
    "train = df_all.iloc[:num_train]\n",
    "test = df_all.iloc[num_train:]\n",
    "id_test = test['id']\n",
    "#balance train\n",
    "c = sqlite3.connect(':memory:')\n",
    "#c = sqlite3.connect('temp.db')\n",
    "train.to_sql('t',c)\n",
    "train = pd.read_sql('select * from t order by relevance desc, product_uid asc', c, index_col=['index'])\n",
    "df_even = train.iloc[::2]  # even\n",
    "df_odd = train.iloc[1::2]  # odd\n",
    "train = pd.concat((df_even, df_odd), axis=0, ignore_index=True)\n",
    "train = train.reset_index(drop=True)\n",
    "y_train = train['relevance']\n",
    "print(\"--- Features Set: %s minutes ---\" % round(((time.time() - start_time)/60),2))\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "RMSE  = make_scorer(fmean_squared_error, greater_is_better=False)\n",
    "\n",
    "class cust_regression_vals(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, hd_searches):\n",
    "        d_col_drops=['id','relevance','search_term','product_title','product_description','product_info','attr','brand','search_and_prod_info','len_of_query', 'len_of_brand', 'query_in_description', 'word_in_brand', 'ratio_brand', 'ft04', 'ft05', 'ftx05', 'ftz04', 'ftz05', 'sft05', 'sftx05', 'sftz05','ft06', 'ft07', 'ft08', 'ft09', 'ft10', 'ft11', 'ft12', 'ft13', 'ft14', 'ftx06', 'ftx07', 'ftx08', 'ftx09', 'ftx10', 'ftx11', 'ftx12', 'ftx13', 'ftx14', 'ftz06', 'ftz07', 'ftz08', 'ftz09', 'ftz10', 'ftz11', 'ftz12', 'ftz13', 'ftz14', 'sft06', 'sft07', 'sft08', 'sft09', 'sft10', 'sft11', 'sft12', 'sft13', 'sft14', 'sft101', 'sft102', 'sft103', 'sft104', 'sft105', 'sft106', 'sft107', 'sft108', 'sft109', 'sft110', 'sft111', 'sft112', 'sft113', 'sft114', 'sftx06', 'sftx07', 'sftx08', 'sftx09', 'sftx10', 'sftx11', 'sftx12', 'sftx13', 'sftx14', 'sftx101', 'sftx102', 'sftx103', 'sftx104', 'sftx105', 'sftx106', 'sftx107', 'sftx108', 'sftx109', 'sftx110', 'sftx111', 'sftx112', 'sftx113', 'sftx114', 'sftz06', 'sftz07', 'sftz08', 'sftz09', 'sftz10', 'sftz11', 'sftz12', 'sftz13', 'sftz14', 'sftz101', 'sftz102', 'sftz103', 'sftz104', 'sftz105', 'sftz106', 'sftz107', 'sftz108', 'sftz109', 'sftz110', 'sftz111', 'sftz112', 'sftz113', 'sftz114']\n",
    "        hd_searches = hd_searches.drop(d_col_drops,axis=1).values\n",
    "        return hd_searches\n",
    "    \n",
    "class cust_txt_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key].apply(str)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate=0.05, \n",
    "                             silent=False, \n",
    "                             objective=\"reg:linear\", \n",
    "                             nthread=-1, \n",
    "                             gamma=0.5, \n",
    "                             min_child_weight=5, \n",
    "                             max_delta_step=1,\n",
    "                             subsample=0.7, \n",
    "                             colsample_bytree=0.7, \n",
    "                             colsample_bylevel=1, \n",
    "                             reg_alpha=0.5, \n",
    "                             reg_lambda=1, \n",
    "                             scale_pos_weight=1,\n",
    "                             base_score=0.5, \n",
    "                             seed=0, \n",
    "                             missing=None)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "tsvd = TruncatedSVD(n_components=50, random_state = 2016)\n",
    "tnmf = NMF(n_components=50, random_state = 2016)\n",
    "clf = pipeline.Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "                    transformer_list = [\n",
    "                        ('cst',  cust_regression_vals()),  \n",
    "                        ('txt2', pipeline.Pipeline([('s2', cust_txt_col(key='search_and_prod_info')), ('tfidf2', tfidf), ('tsvd2', tsvd)])),\n",
    "                        ('txt3', pipeline.Pipeline([('s3', cust_txt_col(key='search_and_prod_info')), ('tfidf3', tfidf), ('tnmf', tnmf)]))\n",
    "                        ],\n",
    "                    transformer_weights = {\n",
    "                        'cst': 1.0,\n",
    "                        'txt2': 1.0,\n",
    "                        'txt3': 1.0\n",
    "                        },\n",
    "                #n_jobs = -1\n",
    "                )), \n",
    "        ('xgb_model', xgb_model)])\n",
    "param_grid = {'xgb_model__n_estimators': [2000], 'xgb_model__max_depth': [10]}\n",
    "model = GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs = -1, cv = 2, verbose = 20, scoring=RMSE)\n",
    "model.fit(train, y_train.values)\n",
    "\n",
    "print(\"----------------------------------\")\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)\n",
    "print(model.best_score_ + 0.452493857808)\n",
    "print(\"----------------------------------\")\n",
    "for i in range(len(model.grid_scores_)):\n",
    "    print(model.grid_scores_[i][0], model.grid_scores_[i][1])\n",
    "    print(model.grid_scores_[i][2])\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "y_pred = model.predict(test)\n",
    "\n",
    "min_y_pred = min(y_pred)\n",
    "max_y_pred = max(y_pred)\n",
    "min_y_train = min(y_train.values)\n",
    "max_y_train = max(y_train.values)\n",
    "print(min_y_pred, max_y_pred, min_y_train, max_y_train)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]<1.0:\n",
    "        y_pred[i] = 1.0\n",
    "    if y_pred[i]>3.0:\n",
    "        y_pred[i] = 3.0\n",
    "    #y_pred[i] = min_y_train + (((y_pred[i] - min_y_pred)/(max_y_pred - min_y_pred))*(max_y_train - min_y_train))\n",
    "pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission_xgb_02.csv',index=False)\n",
    "print(\"--- Training & Testing: %s minutes ---\" % round(((time.time() - start_time)/60),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the queries\n",
    "---------------------------------------------------------------\n",
    "create a dictionary of search terms in 1-3 using sets find the differences and how many times it was used\n",
    "Approach:\n",
    "1) Categorize each word like NLTK does (Noun, verb, ect)\n",
    "2) Based on combination create some sort of conclusion\n",
    "\n",
    "brand, dimension, color, specifications (washer & dryer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
